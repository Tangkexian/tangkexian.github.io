<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?">
  <meta name="keywords" content="Human Preference Alignment, Multimodal Large Language Model, Visual Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <!-- <script defer src="./static/js/fontawesome.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>


</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=cXjomd8AAAAJ&hl=zh-CN&oi=ao"> Kexian Tang</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://jeoyal.github.io/home/">Junyao Gao</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://zengyh1900.github.io">Yanhong Zeng</a><sup>1†</sup>,
            </span>
            <span class="author-block">
              <a href="https://kennymckormick.github.io/">Haodong Duan</a><sup>1†</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6TA1oPkAAAAJ&hl=zh-CN&oi=ao">Yanan Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=sVYO0GYAAAAJ">Zhening Xing</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=fwKOaD8AAAAJ">Wenran Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://kaifeng.ac/cn/">Kaifeng Lyu</a><sup>3‡</sup>,
            </span>
            <span class="author-block">
              <a href="https://chenkai.site/">Kai Chen</a><sup>1‡</sup>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Laboratory</span>
            <span class="author-block"><sup>2</sup>Tongji University</span>
            <span class="author-block"><sup>3</sup>Simons Institute, UC Berkeley</span>
          </div>

          <div class="is-size-6 has-text-centered" style="max-width: 600px; margin: 0 auto; padding-top: 4px;">
            <p><sup>*</sup>Equal contribution, <sup>†</sup>Project Leads, <sup>‡</sup>Corresponding Authors.</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.19990"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tangkexian/LEGO-Puzzles"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://opencompass.openxlab.space/utils/VLMEval/LEGO.tsv"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-download"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Abstract. -->
<section class="hero is-light">
  <div class="container is-max-desktop ">
    <div class="columns is-centered has-text-centered" style="margin-top: 10px; margin-bottom: 0px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Multi-step spatial reasoning</strong> entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce <strong>LEGO-Puzzles</strong>, a scalable benchmark designed to evaluate both <strong>spatial understanding</strong> and <strong>sequential reasoning</strong> in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- /Abstract. -->

<!-- Method. -->
<section class="section" style="margin-top:-50px; margin-bottom:-50px;">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3 is-centered">LEGO-Puzzles Dataset</h2>
            <div class="content has-text-justified">
              <p>
                To comprehensively assess multi-step spatial reasoning in MLLMs, we design <strong>LEGO-Puzzles</strong>, a new benchmark built upon LEGO assembly tasks. Inspired by how humans develop spatial skills through construction processes, we categorize our tasks into three levels: <strong>spatial understanding</strong>, <strong>single-step sequential reasoning</strong>, and <strong>multi-step sequential reasoning</strong>.
              </p>
            </div>
            
            <div class="publication-img">
              <img id="architecture" src="./static/images/teaser.png" style="width:1000px; margin-top:10px;margin-bottom:10px;"/>
            </div>
          </div>
        </div>
        <p>
          <strong>Task examples of LEGO-Puzzles.</strong> From left to right, the columns represent tasks in Spatial Understanding, Single-Step Sequential Reasoning, and Multi-Step Sequential Reasoning. Note: The questions above are slightly simplified for clarity and brevity.
        </p>
    </div>
  </div>
</section>
<!-- Method  -->



<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">     
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Dataset & Benchmark</h2>
          <div class="content has-text-justified">
            <p>
              LEGO-Puzzles contains over <strong>1,100 visual question-answering samples across 11 tasks</strong>. These tasks are evenly distributed across spatial understanding (36.4%), single-step sequential reasoning (36.4%), and multi-step sequential reasoning (27.3%).
            </p>
          </div>
          <div class="publication-img">
            <img id="architecture" src="./static/images/statistic.png" style="width:1000px; margin-top:10px;margin-bottom:10px;"/>
          </div>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        Statistics of LEGO-Puzzles.
      </h2>
    </div>
  </div>
</section>


<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Main Evaluation Results</h2>
          <div class="content has-text-justified">
            <p>
              We evaluate <strong>20 state-of-the-art MLLMs</strong>, including both proprietary and open-source models, across all LEGO-Puzzles tasks. The table below summarizes the performance of these models on spatial understanding, single-step reasoning, and multi-step reasoning tasks. Despite recent advances, most models exhibit significant limitations in reasoning performance, especially when handling spatial rotations, 3D adjacency, or multi-step assembly sequences.
            </p>
          </div>
          <div class="publication-img">
            <img id="architecture" src="./static/images/main_results.png" style="width:1000px; margin-top:10px;margin-bottom:10px;"/>
          </div>
        </div>
      </div>
      <p>
        Table 1. <strong><u>Full Evaluation Results of 18 MLLMs on LEGO-Puzzles.</u></strong>
        <span style="background-color: #888888;">Dark Gray</span>
        indicates the best performance for each task among all models and
        <span style="background-color: #dddddd;">Light Gray</span>
        indicates the best result among open-source model. We also highlight the top three models based on their overall
        performance, using
        <span style="background-color: #00FF00;">Dark Green</span>,
        <span style="background-color: #99FF99;">Medium Green</span>,
        and
        <span style="background-color: #ccffcc;">Light Green</span>, respectively.
      </p>
    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Human vs Model Performance</h2>
          <div class="content has-text-justified">
            <p>
              To further explore the performance gap between humans and MLLMs, we introduce <strong>LEGO-Puzzles-Lite</strong>, a compact subset of the full benchmark with 220 carefully selected samples (20 per task). The following table compares the performance of top-performing MLLMs against human annotators. While humans achieve near-perfect accuracy, even the best models lag significantly, highlighting the challenges MLLMs face in visual and spatial reasoning.
            </p>
          </div>
          <div class="publication-img">
            <img id="architecture" src="./static/images/LEGO-Puzzles-Lite_results.png" style="width:1000px; margin-top:10px;margin-bottom:10px;"/>
          </div>
        </div>
      </div>
      <p>
        Table 2. <strong><u>Comparing Top-Performing MLLMs with Human Proficiency on LEGO-Puzzles-Lite.</u></strong>
        The best results are marked in <strong>bold</strong>.
        The top three overall performances are highlighted in
        <span style="background-color: #00FF00;">Dark Green</span>,
        <span style="background-color: #99FF99;">Medium Green</span>,
        and
        <span style="background-color: #ccffcc;">Light Green</span>, respectively.
      </p>
    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Image Generation Results</h2>
          <div class="content has-text-justified">
            <p>
              In addition to question-answering, LEGO-Puzzles also incorporates <strong>image generation tasks</strong> to evaluate whether MLLMs can visually interpret and simulate spatial transformations. We design <strong>5 generation tasks</strong> across two main categories: spatial understanding (<em>Rotation*</em>, <em>Multiview*</em>) and single-step sequential reasoning (<em>Position*</em>, <em>Dependency*</em>, <em>Next-Step*</em>). Each task consists of 20 questions, resulting in a total of 100 generation questions. Models are expected to generate an image of the intermediate LEGO configuration that reflects the given instruction. Because standard automated metrics fail to assess spatial fidelity and reasoning consistency, we rely on human evaluation to rate two aspects: <strong>appearance similarity</strong> and <strong>instruction following</strong>.
            </p>
            <p>
              Only Gemini-2.0-Flash and GPT-4o show limited success, with most open-source models failing entirely to follow
              instructions or preserve structural identity. This reveals critical limitations in spatially grounded image synthesis
              within current MLLMs.
            </p>
          </div>
          <div class="publication-img">
            <img id="architecture" src="./static/images/Generation_results.png"
              style="width:1000px; margin-top:10px;margin-bottom:10px;" />
          </div>
        </div>
      </div>
      <p>
        <strong>Table 3. <u>Evaluation on <em>Generation</em>.</u></strong>
        We conduct human-based evaluation to assess the “Appearance” (App) and “Instruction Following” (IF) scores of
        Gemini-2.0-Flash, GPT-4o, Emu2, GILL, and Anole, using a scoring scale from 0 to 3 for both dimensions.
      </p>
    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Multi-Step Reasoning via Next-k-Step</h2>
          <div class="content has-text-justified">
            <p>
              To analyze model behavior under increased reasoning depth, we introduce <strong>Next-k-Step</strong>, a fine-grained extension of our sequential reasoning tasks. It requires predicting the correct assembly result after <em>k</em> consecutive steps. We evaluate performance as <em>k</em> increases (1–5), with and without <strong>Chain-of-Thought (CoT)</strong> prompting. The results below reveal that most models suffer from performance degradation with more steps, and CoT prompting does not consistently help, especially in long-range spatial reasoning.
            </p>
            </p>
          </div>
          <div class="publication-img">
            <img id="architecture" src="./static/images/next-k-step_results.png"
              style="width:1000px; margin-top:10px;margin-bottom:10px;" />
          </div>
        </div>
      </div>
      <p>
        <strong>Table 4. <u>Evaluation on <em>Next-k-Step</em>.</u></strong>
        <em>k</em> represents the number of steps, and CoT refers to adding a “Think step by step before answering”
        instruction in QA pairs, similar to those in LLMs.
      </p>
    </div>
  </div>
</section>





<section>
  <div class="container is-max-desktop" style="margin-top:-50px; margin-bottom:-50px;">
    <div class="hero-body">      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Samples</h2>
          <div class="image-grid">
            <div class="image-item">
              <img src="./static/images/sample1.png" alt="Sample 1" style="width: 100%; margin-top: 10px; margin-bottom: 10px;" />
            </div>
            <div class="image-item">
              <img src="./static/images/sample2.png" alt="Sample 2" style="width: 100%; margin-top: 10px; margin-bottom: 10px;" />
            </div>
          </div>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        Samples of tasks in LEGO-Puzzles dataset.
      </h2>
    </div>
  </div>
</section>

<style>
  .image-grid {
    display: grid;
    grid-template-columns: repeat(2, 1fr);
    gap: 20px; 
  }


  .image-item img {
    width: 100%; 
    height: auto; 
    display: block; 
    margin-top: 10px;
    margin-bottom: 10px;
  }
</style>


<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tang2025lego,
    title={LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?},
    author={Tang, Kexian and Gao, Junyao and Zeng, Yanhong and Duan, Haodong and Sun, Yanan and Xing, Zhening and Liu,
    Wenran and Lyu, Kaifeng and Chen, Kai},
    journal={arXiv preprint arXiv:2503.19990},
    year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. Thanks for their excellent work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
